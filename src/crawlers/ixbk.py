"""
çº¿æŠ¥é…·çˆ¬è™« - https://new.ixbk.net/
"""
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import re
import asyncio
from .base import BaseCrawler


class IxbkCrawler(BaseCrawler):
    """çº¿æŠ¥é…·çˆ¬è™«"""
    
    def __init__(self, fetch_detail: bool = True):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            fetch_detail: æ˜¯å¦çˆ¬å–è¯¦æƒ…é¡µè·å–å®Œæ•´å†…å®¹ï¼ˆé»˜è®¤Trueï¼‰
        """
        super().__init__()
        self.source_name = "çº¿æŠ¥é…·"
        self.base_url = "https://new.ixbk.net/"
        self.fetch_detail = fetch_detail
        
    async def crawl(self) -> List[Dict]:
        """çˆ¬å–çº¿æŠ¥é…·é¦–é¡µå†…å®¹"""
        try:
            # è·å–é¦–é¡µHTML
            html = await self.fetch_page(self.base_url)
            if not html:
                self.logger.error("è·å–çº¿æŠ¥é…·é¡µé¢å¤±è´¥")
                return []
            
            soup = self.parse_html(html)
            posts = []
            
            # æ‰¾åˆ°æ‰€æœ‰æ–‡ç« åˆ—è¡¨é¡¹
            article_list = soup.find('ul', class_='new-post')
            if not article_list:
                self.logger.error("æœªæ‰¾åˆ°æ–‡ç« åˆ—è¡¨")
                return []
            
            articles = article_list.find_all('li', class_='article-list')
            self.logger.info(f"æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            
            for article in articles:
                try:
                    post = await self._parse_article(article)
                    if post:
                        posts.append(post)
                except Exception as e:
                    self.logger.error(f"è§£ææ–‡ç« å¤±è´¥: {e}")
                    continue
            
            self.logger.info(f"æˆåŠŸè§£æ {len(posts)} ç¯‡æ–‡ç« ")
            return posts
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–çº¿æŠ¥é…·å¤±è´¥: {e}")
            return []
    
    async def _parse_article(self, article) -> Optional[Dict]:
        """è§£æå•ç¯‡æ–‡ç« """
        try:
            # è·å–æ ‡é¢˜å’Œé“¾æ¥
            title_element = article.find('a')
            if not title_element:
                return None
            
            title = title_element.get('title', '').strip()
            link = title_element.get('href', '').strip()
            
            if not title or not link:
                return None
            
            # è¡¥å…¨é“¾æ¥
            if link.startswith('/'):
                link = f"https://new.ixbk.net{link}"
            
            # è·å–æ—¶é—´
            time_element = article.find('time', class_='badge')
            time_str = time_element.get_text().strip() if time_element else ""
            
            # è·å–åˆ†ç±»
            category = title_element.get('data-catename', 'æœªåˆ†ç±»')
            
            # è·å–å†…å®¹
            content = title_element.get('data-content', '').strip()
            
            # è·å–è¯„è®ºæ•°
            comment_element = article.find('span', class_='badge com')
            comments = 0
            if comment_element:
                comment_text = comment_element.get_text().strip()
                # æå–æ•°å­—
                match = re.search(r'\d+', comment_text)
                if match:
                    comments = int(match.group())
            
            # è·å–å‘å¸ƒè€…
            author = title_element.get('data-louzhu', 'åŒ¿å')
            
            # è§£ææ—¶é—´
            pub_date = self._parse_time(time_str)
            
            # å¦‚æœå¯ç”¨è¯¦æƒ…é¡µæŠ“å–ï¼Œè·å–å®Œæ•´å†…å®¹
            full_content = content or title
            if self.fetch_detail:
                detail_content = await self._fetch_detail_content(link)
                if detail_content:
                    full_content = detail_content
            
            # åˆ›å»ºæ–‡ç« å­—å…¸
            post = self.create_post_dict(
                title=title,
                url=link,
                author=author,
                publish_time=pub_date,
                content=full_content
            )
            
            # æ·»åŠ åˆ†ç±»ä¿¡æ¯
            post['category'] = category
            
            # æ·»åŠ é¢å¤–ä¿¡æ¯
            post['comments'] = comments
            
            return post
            
        except Exception as e:
            self.logger.error(f"è§£ææ–‡ç« å…ƒç´ å¤±è´¥: {e}")
            return None
    
    def _parse_time(self, time_str: str) -> datetime:
        """
        è§£ææ—¶é—´å­—ç¬¦ä¸²
        æ”¯æŒæ ¼å¼ï¼š
        - "11:00" -> ä»Šå¤©11:00
        - "10:59" -> ä»Šå¤©10:59
        """
        try:
            if not time_str:
                return datetime.now()
            
            # åŒ¹é… HH:MM æ ¼å¼
            match = re.match(r'(\d{1,2}):(\d{2})', time_str)
            if match:
                hour = int(match.group(1))
                minute = int(match.group(2))
                
                now = datetime.now()
                pub_date = now.replace(hour=hour, minute=minute, second=0, microsecond=0)
                
                # å¦‚æœæ—¶é—´æ¯”ç°åœ¨æ™šï¼Œè¯´æ˜æ˜¯æ˜¨å¤©çš„
                if pub_date > now:
                    pub_date = pub_date - timedelta(days=1)
                
                return pub_date
            
            # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›å½“å‰æ—¶é—´
            return datetime.now()
            
        except Exception as e:
            self.logger.error(f"è§£ææ—¶é—´å¤±è´¥: {time_str}, {e}")
            return datetime.now()
    
    async def _fetch_detail_content(self, url: str) -> Optional[str]:
        """
        è·å–è¯¦æƒ…é¡µçš„æ ¸å¿ƒå†…å®¹ä¿¡æ¯ï¼ˆåŒ…å«è¯„è®ºåŒºçš„é“¾æ¥å’Œè·å–æ–¹æ³•ï¼‰
        
        Args:
            url: è¯¦æƒ…é¡µURL
            
        Returns:
            æ ¼å¼åŒ–çš„æ ¸å¿ƒå†…å®¹ï¼ˆæ­£æ–‡+åŸæ–‡é“¾æ¥+è¯„è®ºåŒºé“¾æ¥ï¼‰
        """
        try:
            # è·å–è¯¦æƒ…é¡µHTML
            html = await self.fetch_page(url)
            if not html:
                return None
            
            soup = self.parse_html(html)
            content_parts = []
            
            # 1. è·å–æ–‡ç« æ­£æ–‡ï¼ˆæ ¸å¿ƒçº¿æŠ¥ä¿¡æ¯ï¼‰
            article_content = soup.find('div', class_='article-content')
            if article_content:
                text = article_content.get_text('\n', strip=True)
                if text:
                    content_parts.append(text)
            
            # 2. è·å–åŸæ–‡è´­ä¹°é“¾æ¥
            source_link = soup.find('a', text=lambda t: t and 'åŸæ–‡åœ°å€' in t)
            if source_link:
                href = source_link.get('href', '')
                if href:
                    content_parts.append(f"\nğŸ”— åŸæ–‡é“¾æ¥: {href}")
            
            # 3. æå–è¯„è®ºåŒºçš„é“¾æ¥å’Œè·å–æ–¹æ³•
            comment_links = self._extract_comment_links(soup)
            if comment_links:
                content_parts.append("\n\nğŸ’¬ è¯„è®ºåŒºè¡¥å……:")
                content_parts.append(comment_links)
            
            if content_parts:
                return '\n\n'.join(content_parts)
            
            return None
            
        except Exception as e:
            self.logger.error(f"è·å–è¯¦æƒ…é¡µå†…å®¹å¤±è´¥ {url}: {e}")
            return None
    
    def _extract_comment_links(self, soup) -> str:
        """
        ä»è¯„è®ºåŒºæå–å•†å“é“¾æ¥å’Œè·å–æ–¹æ³•
        
        Args:
            soup: BeautifulSoupå¯¹è±¡
            
        Returns:
            æ ¼å¼åŒ–çš„è¯„è®ºé“¾æ¥ä¿¡æ¯
        """
        try:
            links_info = []
            
            # æŸ¥æ‰¾è¯„è®ºåˆ—è¡¨
            comment_list = soup.find('div', class_='comment-list')
            if not comment_list:
                return ""
            
            # æŸ¥æ‰¾æ‰€æœ‰è¯„è®ºå®¹å™¨
            comment_uls = comment_list.find_all('div', class_='ul')
            
            for i, ul in enumerate(comment_uls[:10], 1):  # æœ€å¤šå–10æ¡è¯„è®º
                try:
                    li = ul.find('div', class_='li')
                    if not li:
                        continue
                    
                    # è·å–è¯„è®ºå†…å®¹
                    content_elem = li.find('div', class_='c-neirong')
                    if not content_elem:
                        continue
                    
                    comment_text = content_elem.get_text().strip()
                    
                    # æå–è¯„è®ºä¸­çš„æ‰€æœ‰é“¾æ¥
                    links = content_elem.find_all('a')
                    for link in links:
                        href = link.get('href', '')
                        link_text = link.get_text().strip()
                        if href:
                            # è®°å½•é“¾æ¥å’Œä¸Šä¸‹æ–‡
                            links_info.append(f"[{i}] {link_text}: {href}")
                    
                    # å¦‚æœè¯„è®ºä¸­æ²¡æœ‰<a>æ ‡ç­¾ï¼Œä½†åŒ…å«URLæ–‡æœ¬
                    if not links:
                        # ä½¿ç”¨æ­£åˆ™æå–URL
                        import re
                        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
                        urls = re.findall(url_pattern, comment_text)
                        if urls:
                            for url in urls:
                                links_info.append(f"[{i}] {url}")
                        # ä¹Ÿæå–åŒ…å«è·å–æ–¹æ³•çš„å…³é”®ä¿¡æ¯
                        elif any(keyword in comment_text for keyword in ['å£ä»¤', 'å¯†ä»¤', 'é“¾æ¥', 'è¿›å…¥', 'æœç´¢', 'æ‰“å¼€']):
                            # è¿™æ¡è¯„è®ºå¯èƒ½åŒ…å«è·å–æ–¹æ³•
                            if len(comment_text) < 200:  # åªä¿ç•™è¾ƒçŸ­çš„è¯´æ˜
                                links_info.append(f"[{i}] {comment_text}")
                        
                except Exception as e:
                    self.logger.debug(f"å¤„ç†å•æ¡è¯„è®ºå¤±è´¥: {e}")
                    continue
            
            if links_info:
                return '\n'.join(links_info)
            
            return ""
            
        except Exception as e:
            self.logger.error(f"æå–è¯„è®ºé“¾æ¥å¤±è´¥: {e}")
            return ""
    